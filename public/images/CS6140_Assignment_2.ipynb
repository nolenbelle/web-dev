{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c0259765",
      "metadata": {
        "id": "c0259765"
      },
      "source": [
        "# CS 6140 Machine Learning: Assignment - 2  \n",
        "\n",
        "## Total Points: 100\n",
        "\n",
        "## Prof. Ahmad Uzair\n",
        "\n",
        "This assignment requires implementation from scratch. You can use other libraries to tally your results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d22122e",
      "metadata": {
        "id": "4d22122e"
      },
      "source": [
        "### Problem 1. (20 points)\n",
        "\n",
        "Consider a logistic regression problem where $\\mathcal{X} = \\mathbb{R}^d$ and $\\mathcal{Y} = \\left\\{ -1, +1\\right\\}$. Derive the weight update rule that maximizes the conditional likelihood assuming that a data set $\\mathcal{D}=\\left\\{ (\\boldsymbol{x}_{i},y_{i})\\right\\} _{i=1}^{n}$ is given."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca5d8dd3",
      "metadata": {
        "id": "ca5d8dd3"
      },
      "source": [
        "### Problem 2. (20 points) \n",
        "The sigmoid function is given as: \n",
        "    $\\sigma(a)=\\frac{1}{1+e^{-a}}$\n",
        " Solve the following questions.\n",
        "\n",
        "a. (5 points) Compute $\\frac{d\\sigma(a)}{dx} $ when $\\ a = w^Tx $, where $\\ w, x \\hspace{2 pt} \\in \\hspace{2 pt} \\mathbb{R}^m $\n",
        "\n",
        "b. (5 points) For logistic regression with target variable $ y_i \\hspace{2pt} \\epsilon \\hspace{2 pt} \\{âˆ’1, 1\\}$, the posterior probability of the positive class is:\n",
        "\\begin{equation}\n",
        "    P(y = 1|x,w)=\\sigma(w^Tx) =\\frac{1}{1 + e^{-w^Tx}}\n",
        "\\end{equation}\n",
        "\n",
        "Show that we can express the posterior for both classes as:\n",
        "\n",
        "\\begin{equation}\n",
        "    P(y = \\pm 1|x,w)= \\sigma(w^Tx)= \\frac{1}{1 + e^{-yw^Tx}}\n",
        "\\end{equation}\n",
        "\n",
        "c. (10 points) Show that the loss function for logistic regression is:\n",
        "\n",
        "\\begin{equation}\n",
        "    L_{log} = \\sum\\limits_{i=1}^N \\log(1 + e^{-y_iw^Tx_i})\n",
        "\\end{equation}\n",
        "\n",
        "where, $x_i , w \\hspace{2 pt} \\epsilon \\hspace{2 pt} \\mathbb{R}^m \\hspace{2 pt}and\\hspace{2 pt} y_i \\hspace{2 pt}\\in \\hspace{2 pt} \\{\\pm 1\\}$\n",
        "\n",
        "We can see that the expression in the exponent i.e., $\\ y_iw^Tx_i $ is a product of the given training label $\\ y_i$ and the dot product of the weight vector with the input feature vector $\\ g(x_i, w) = w^Tx_i$. Please explain how the loss function behaves when the training label and dot product have the same sign (positive/negative) and when they differ."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ca0ce4f",
      "metadata": {
        "id": "0ca0ce4f"
      },
      "source": [
        "### Q4. Perceptron & Logistic Regression  (60 points)\n",
        "In this problem you will be applying logistic regression to the breastcancer dataset for binary classification:\n",
        "\n",
        "**Breast Cancer**:  this dataset is aimed at developing classifiers that can distinguish be-tween malignant and benign tumors in breast cancer.   There are thirty real valued features and 569 instances.\n",
        "\n",
        "The dataset can be accessed here: <html>https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)</html>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdbee24d",
      "metadata": {
        "id": "fdbee24d"
      },
      "source": [
        "### Task\n",
        "a. Implement perceptron with bias on the dataset. \n",
        "\n",
        "b. Implement logistic regression using gradient descent. Do ten-fold cross validation and find the best step-size $\\alpha$, using accuracy as the metric. \n",
        "\n",
        "c. Finally, use the best $\\alpha$ you get and re-train both the model on the full training data, and report the accuracy, precision and recall.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afd46f98",
      "metadata": {
        "id": "afd46f98"
      },
      "source": [
        "Rubrix for Problem 4:\n",
        "\n",
        "a. Implementation of perceptron with weight update function. (10 points)\n",
        "\n",
        "b. Implementation of loss function and sigmoid (10 points)\n",
        "\n",
        "c. Implementation of Gradient descent & evaluation (10 points)\n",
        "\n",
        "d. Implementation of cross validation (10 points)\n",
        "\n",
        "e. Retraining perceptron and logistic regression with whole training set with selected alpha value  (10 points)\n",
        "\n",
        "f. Compare the results of logistic regression with that of perceptron. (10  points)\n",
        "\n",
        "<b>As a reminder all functions for this problem have to be coded from scratch</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5082375d",
      "metadata": {
        "id": "5082375d"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "colab": {
      "name": "CS6140 Assignment 2.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}